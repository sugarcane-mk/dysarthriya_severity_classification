# ğŸ§  Dysarthria Severity Classification

**Classify dysarthric speech severity using Wav2Vec2 and DistilAlHuBERT embeddings.**

---

## ğŸ“Œ Overview

Dysarthria is a motor speech disorder that affects articulation and intelligibility. This project aims to **automatically classify the severity level** of dysarthric speech (e.g., *mild*, *moderate*, *severe*) using pre-trained **self-supervised speech models** â€” **Wav2Vec2** and **DistilAlHuBERT** â€” for feature extraction, followed by deep learning or traditional models for classification.

---

## ğŸ“‚ Datasets

### âœ… Tamil Dysarthric Speech Corpus (SSN-TDSC)

* **Language**: Tamil
* **Content**: 365 phonetically balanced sentences and isolated-words
* **Speakers**: Mild and moderate dysarthric speakers
* **Format**: `.wav` audio files with corresponding labels

### âœ… UA-Speech

* **Language**: English
* **Content**: Isolated word corpus from dysarthric and control speakers
* **Speakers**: 15 dysarthric and 13 control speakers

### âœ… TORGO Corpus

* **Language**: English
* **Content**: Sentences, isolated words, and paragraph-level recordings
* **Speakers**: 8 Dysarthric and 7 control speakers with metadata for severity

> ğŸ” *These datasets were collected under ethical guidelines. Ensure proper attribution and usage rights.*

---

## ğŸ§  Methodology

### 1. **Feature Extraction**

* **Wav2Vec2**: Extracted using `facebook/wav2vec2-base`
* **DistilAlHuBERT**: Extracted using `voidful/distilalhubert`
* Output: `.npy` files and `.pt`Â  filesÂ containing embeddings per utterance

### 2. **Preprocessing**

* Mean pooling across frames to get a fixed-size embedding
* Normalization

### 3. **Classification**

* Models used:

  * Support Vector Machines (SVM)
  * Convolutional Neural Networks (CNN)
* Hyperparameter tuning (e.g., `C`, `gamma` for SVM; learning rate, dropout for CNN)
* Evaluation:

  * Accuracy
  * F1-score
  * Confusion Matrix

### 4. **Cross-Validation Strategy**

* **Leave-One-Speaker-Out (LOSO)** cross-validation used to evaluate generalizability
* For each iteration:

  * One speaker is held out for testing
  * Remaining speakers used for training
  * Metrics averaged across all iterations

---

## ğŸ§  Cross-Dataset Strategy

* Independent training on datasets: SSN-TDSC, UA-Speech, TORGO
* Balanced data preprocessing across corpora
* Domain adaptation explored for generalization across languages

---

## ğŸ“Š Results

| Model | Feature SetÂ Â   | Accuracy | F1-Score |
| ----- | -------------- | -------- | -------- |
| SVM   | Wav2Vec2       |   |      |
| CNN   | Wav2Vec2       |   |      |
| SVM   | DistilAlHuBERT |    |     |
| CNN   | DistilAlHuBERT |    |     |

> ğŸ“ˆ Best performance achieved using **Wav2Vec2 + CNN** with **LOSO** cross-validation

---

## ğŸ“œ Folder Structure

```
dysarthria_severity_classification/
â”‚
â”œâ”€â”€ audio/                  # Raw audio files
â”œâ”€â”€ embeddings/             # Extracted embeddings (.npy)
â”œâ”€â”€ labels.csv              # Severity labels
â”œâ”€â”€ extract_embeddings.py   # Embedding extraction script
â”œâ”€â”€ train_classifier.py     # Model training and evaluation
â”œâ”€â”€ utils.py                # Helper functions
â””â”€â”€ README.md
```

---

## ğŸ¤ Acknowledgements

* **SSN-TDSC**, **UA-Speech**, and **TORGO** corpora
* **Facebook AI** for Wav2Vec2
* **Voidful** for DistilAlHuBERT
* **NIEPMD** for clinical guidance
